{"cells":[{"cell_type":"code","source":["import pyspark\ndf = sqlContext.sql(\"SELECT * FROM appl_stock_csv\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.head(3)[0]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# filter data\n\ndf.filter(\"Close < 500\").select('open').show()    # select([]) to show multiple columns"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Using python syntax\n\ndf.filter(df['CLose'] < 500).select('Volume').show()   "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# multiple filters\n\n\ndf.filter( (df['Close']<200) & (df['Open']>200) ).show()    \n\n# NEED () to separate them. and does not work use | or &\n# and not : & ~  \n# ! is ~ in pyspark"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# find specific data\n\ndf.filter(df['Low'] == 197.16).show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df.filter(df['Low'] == 197.16).collect()    # notice you get list"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["result = df.filter(df['Low'] == 197.16).collect()\nresult"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["row = result[0]"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["row.asDict()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["row.asDict()['Volume']"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"Spark Data Frame Operation","notebookId":1265109201675384},"nbformat":4,"nbformat_minor":0}
