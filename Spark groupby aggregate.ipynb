{"cells":[{"cell_type":"code","source":["import pyspark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = sqlContext.sql(\"SELECT * FROM sales_info_csv\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df.groupBy('Company')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.groupBy('Company').mean().show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# to see how many rows each company has\n\ndf.groupBy('Company').count().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.agg({'Sales':'sum'}).show()       # {'column name' : 'aggregate function'}"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# groupby using dictionary syntax\n# especially good with for loops\n\ngroup_data = df.groupBy('Company')\ngroup_data.agg({'Sales':'max'}).show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct,avg,stddev     # various functions available"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.select(countDistinct('Sales')).show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.select(avg('Sales')).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# change column name\n\ndf.select(avg('Sales').alias('Average Sales')).show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df.select(stddev('Sales')).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import format_number"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df.select(stddev('Sales').alias('std')).show()\n\n# notice its too long. Lets fix this"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["sales_std = df.select(stddev('Sales').alias('std'))\nsales_std.select(format_number('std', 2)).show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["sales_std.select(format_number('std', 2).alias('std')).show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df.orderBy('Sales').show()     # default is ascending"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df.orderBy(df['Sales'].desc()).show()      # to make it descending"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"Spark groupby aggregate","notebookId":1265109201675399},"nbformat":4,"nbformat_minor":0}
